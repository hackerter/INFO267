{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material teórico\n",
    "\n",
    "[Slides](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento digital de imágenes\n",
    "\n",
    "Una imagen es una colección de pixeles ordenados\n",
    "\n",
    "En estándar RGB cada pixel corresponde a 3 valores enteros de 8 bit (256 niveles). Combinándolos formamos colores (aproximadamente 16.7M)\n",
    "\n",
    "Otra codificación usual para los pixeles consiste en usar un número entre cero y uno para cada canal (color)\n",
    "\n",
    "El estándar RGBA añade un canal que representa la opacidad\n",
    "\n",
    "Las imágenes en escala de grises y sin opacidad se pueden representar usando un canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('cameraman.png')\n",
    "img_bw = 0.2989*img[:, :, 0] + 0.587*img[:, :, 1]+ 0.114*img[:, :, 2]\n",
    "\n",
    "display(img_bw.shape)\n",
    "display(img_bw.dtype)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img_bw, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestros sistemas digitales una imagen es una arreglo multidimensional y podemos operarlo como tal\n",
    "\n",
    "A que corresponde este segmento del arreglo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subimg = np.copy(img_bw[50:100, 120:180])\n",
    "display(subimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(subimg, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y este segmento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(6, 3))\n",
    "ax[1].plot(subimg[30, :])\n",
    "ax[0].imshow(subimg[30:31, :], cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución  y correlación cruzada discreta\n",
    "\n",
    "Una herramienta clásica de procesamiento digital de señales es la **convolución**\n",
    "\n",
    "La operación de convolución entre dos señales unidimensionales discretas es\n",
    "\n",
    "$$\n",
    "(f * g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[n-m]\n",
    "$$\n",
    "\n",
    "y la operación de correlación cruzada es\n",
    "\n",
    "$$\n",
    "(f \\star g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[m+n]\n",
    "$$\n",
    "\n",
    "> Para ambas operaciones el resultado es una nueva señal que también depende de  $n$\n",
    "\n",
    "\n",
    "\n",
    "Por ejemplo el elemento $0$ de $f\\star g$ se calcula como\n",
    "\n",
    "    f[0] g[0] + f[1] g[1] + f[2] g[2] + ...\n",
    "\n",
    "Luego el elemento $1$ sería\n",
    "\n",
    "    f[0] g[1] + f[1] g[2] + f[2] g[3] + ...\n",
    "    \n",
    "¿Cómo se ve esta operación graficamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(2, figsize=(7, 4))\n",
    "ax2 = ax[0].twinx()\n",
    "data = subimg[0, :]\n",
    "\n",
    "def filt(k):\n",
    "    kernel = np.zeros(shape=(len(data),))\n",
    "    kernel[k:k+5] = 1\n",
    "    #kernel[k] = 1.; kernel[k+1] = -1.\n",
    "    return kernel\n",
    "\n",
    "true_filt = filt(0)[np.absolute(filt(0)) >0]\n",
    "display(true_filt)\n",
    "conv_s = scipy.signal.correlate(data, true_filt, mode='valid')\n",
    "\n",
    "\n",
    "def update(k): \n",
    "    ax[0].cla(); ax[1].cla(); ax2.cla();\n",
    "    ax[0].plot(data)\n",
    "    ax2.plot(filt(k), c='r')\n",
    "    ax[1].plot(conv_s); \n",
    "    ax[1].scatter(k, conv_s[k], s=100, c='k')\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=len(conv_s), interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de imágenes con convoluciones\n",
    "\n",
    "Se puede extender el concepto de convolución a dos dimensiones\n",
    "$$\n",
    "(I_1 * I_2) [n_1, n_2] = \\sum_{m_1=-\\infty}^\\infty \\sum_{m_2=-\\infty}^\\infty I_1[m_1, m_2] I_2[n_1-m_2, n_2 - m_2]\n",
    "$$\n",
    "\n",
    "donde $n_1$ es el índice de las filas y $n_2$ es el índice de las columnas\n",
    "\n",
    "#### La convolución entre dos imágenes es una nueva imagen\n",
    "\n",
    "La imagen $I_1$ es la entrada\n",
    "\n",
    "La imagen $I_2$ se denomina filtro o kernel de la convolución\n",
    "\n",
    "La imagen resultante es la imagen filtrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-bajo\n",
    "\n",
    "Suaviza, elimina los detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "D = 3\n",
    "filt = np.zeros(shape=(D, D))\n",
    "filt[1:-1, 1:-1] = 1\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt/np.sum(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-alto\n",
    "\n",
    "Resalta los cambios bruscos, elimina las partes \"planas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.array([[1., -1.]]*2)\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt, mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector de patillas\n",
    "\n",
    "Detecta patillas de fotografos mirando al horizonte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.ones(shape=(11, 11))\n",
    "filt[:9, 2:9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_res = scipy.signal.correlate2d(subimg, filt-np.mean(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "maxloc = np.unravel_index(np.argmax(img_res), shape=img_res.shape)\n",
    "ax[0].scatter(maxloc[1]+filt.shape[0]//2, maxloc[0]+filt.shape[1]//2, c='r', s=20)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Podríamos aprender filtros para detectar objetos específicos\n",
    "\n",
    "> Necesitamos aprender los valores de los \"píxeles\" del kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html)\n",
    "\n",
    "Es una librería utilitaria de PyTorch que facilita considerablemente el trabajo con imágenes\n",
    "\n",
    "- Sets de benchmark incluidos\n",
    "- Modelos clásicos pre-entrenados\n",
    "- Funciones para importar imágenes en distintos formatos\n",
    "- Funciones de transformación para hacer aumentación de datos en imágenes\n",
    "\n",
    "\n",
    "\n",
    "    pip3 install torchvision \n",
    "\n",
    "#### Cargando dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                              train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                              train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "image, label = mnist_train_data[0]\n",
    "display(len(mnist_train_data), type(image), type(label))\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 2), tight_layout=True)\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[k]\n",
    "    ax[k].imshow(image[0, :, :].numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(mnist_train_data.data, mnist_train_data.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "#train_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_dataset = Subset(mnist_train_data, train_idx)\n",
    "#train_data.transforms = train_transforms\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Data loader de validación\n",
    "#valid_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "valid_dataset = Subset(mnist_train_data, valid_idx)\n",
    "#train_data.transforms = train_transforms\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional en PyTorch\n",
    "\n",
    "Las redes neuronales convolucionales tienen tres tipos de capas\n",
    "\n",
    "- Capas convolucionales\n",
    "    \n",
    "        torch.nn.Conv2D\n",
    "        \n",
    "- Capas de pooling\n",
    "\n",
    "        torch.nn.MaxPool2D\n",
    "        \n",
    "- Capas completamente conectadas\n",
    "\n",
    "        torch.nn.Linear\n",
    "        \n",
    "        \n",
    "> Las capas convolucionales ordenan sus neuronas de tal forma que el resultado es una convolución sobre la imagen\n",
    "\n",
    "Los parámetros ajustables más importantes de las capas convolucionales son el \n",
    "- número de filtros\n",
    "- tamaño del kernel \n",
    "- paso (stride) del kernel\n",
    "\n",
    "A diferencia de las capas completamente conectadas no es necesario tener tantas neuronas como entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class mi_red_convolucional(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mi_red_convolucional, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.mpool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3)\n",
    "        self.mpool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=8*5*5, out_features=10)\n",
    "        self.fc2 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.mpool1(self.activation(self.conv1(x)))\n",
    "        z = self.mpool2(self.activation(self.conv2(z)))\n",
    "        z = z.reshape(-1, 8*5*5)\n",
    "        z = self.activation(self.fc1(z))\n",
    "        return self.fc2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mi_red_convolucional()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    nnet = nnet.cuda()\n",
    "\n",
    "# Hiddenlayer objects to track metrics\n",
    "import hiddenlayer as hl\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "## tensorboard --logdir=/tmp/tensorboard\n",
    "#writer = SummaryWriter(\"/tmp/tensorboard/net1\")\n",
    "\n",
    "nepochs = 5\n",
    "epoch_loss = np.zeros(nepochs, 2)\n",
    "epoch_acc = np.zeros(nepochs, 2)\n",
    "for epoch in range(nepochs): \n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    # Train\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = model.forward(mbdata)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss[k, 0] += loss.item()\n",
    "        epoch_acc[k, 0] += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss[k, 0] = epoch_loss[k, 0]/len(train_idx)\n",
    "    epoch_acc[k, 0] = epoch_acc[k, 0]/len(train_idx)\n",
    "    # Validation\n",
    "    #writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), epoch)\n",
    "    #writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), epoch)\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = model.forward(mbdata)\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss[k, 1] += loss.item()\n",
    "        epoch_acc[k, 1] += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    #writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), epoch)\n",
    "    #writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), epoch)\n",
    "    epoch_loss[k, 1] = epoch_loss[k, 0]/len(valid_idx)\n",
    "    epoch_acc[k, 1] = epoch_acc[k, 0]/len(valid_idx)\n",
    "    history1.log(epoch, loss=epoch_loss[k, 1], accuracy=epoch_acc[k, 1])\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss\"]])\n",
    "        canvas1.draw_plot([history1[\"accuracy\"]])\n",
    "    #time.sleep(0.1)\n",
    "\n",
    "if use_gpu:\n",
    "    nnet = model.cpu()\n",
    "    \n",
    "#writer.add_graph(nnet)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando los filtros aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(7, 2))\n",
    "w = model.conv1.weight.data.numpy()\n",
    "\n",
    "for i in range(8):    \n",
    "    ax[i].imshow(w[i, 0, :, :])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando la red neuronal para clasificar ejemplos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_test_data[10]\n",
    "y = torch.nn.Softmax(dim=1)(model.forward(image.unsqueeze(0)))\n",
    "display(y)\n",
    "display(torch.argmax(y))\n",
    "display(label)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = []\n",
    "for i in range(len(mnist_test_data)):\n",
    "    image_test, label_test = mnist_test_data[i]\n",
    "    sl = model.forward(image_test.unsqueeze(0))\n",
    "    y = torch.nn.Softmax(dim=1)(sl)\n",
    "    entropy.append(-(y.exp()*y).sum().detach().numpy())  \n",
    "    \n",
    "d = np.argmax(np.array(entropy))\n",
    "print(d, entropy[d])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La predicción para el ejemplo más incierto:\n",
    "image_test, label_test = mnist_test_data[d]\n",
    "sl = model.forward(image_test.unsqueeze(0))\n",
    "y = torch.nn.Softmax(dim=1)(sl)\n",
    "display(y.argmax())\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(image_test[0, :, :].numpy(), cmap=plt.cm.Greys_r);\n",
    "plt.title(label_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumentación de datos\n",
    "\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Aprendizaje\n",
    "\n",
    "Ajuste fino de un modelo pre-entrenado\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localización y segmentación\n",
    "\n",
    "Encontrar y segmentar objetos en imágenes\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
