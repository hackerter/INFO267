{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento digital de imágenes\n",
    "\n",
    "Una imagen es una colección de pixeles ordenados\n",
    "\n",
    "En estándar RGB cada pixel corresponde a 3 valores enteros de 8 bit (256 niveles). Combinándolos formamos colores (aproximadamente 16.7M)\n",
    "\n",
    "Otra codificación usual para los pixeles consiste en usar un número entre cero y uno para cada canal (color)\n",
    "\n",
    "El estándar RGBA añade un canal que representa la opacidad\n",
    "\n",
    "Las imágenes en escala de grises y sin opacidad se pueden representar usando un canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('cameraman.png')\n",
    "img_bw = 0.2989*img[:, :, 0] + 0.587*img[:, :, 1]+ 0.114*img[:, :, 2]\n",
    "\n",
    "display(img_bw.shape)\n",
    "display(img_bw.dtype)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img_bw, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestros sistemas digitales una imagen es una arreglo multidimensional y podemos operarlo como tal\n",
    "\n",
    "A que corresponde este segmento del arreglo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subimg = np.copy(img_bw[50:100, 120:180])\n",
    "display(subimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(subimg, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y este segmento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(6, 3))\n",
    "ax[1].plot(subimg[30, :])\n",
    "ax[0].imshow(subimg[30:31, :], cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución  y correlación cruzada discreta\n",
    "\n",
    "Una herramienta clásica de procesamiento digital de señales es la **convolución**\n",
    "\n",
    "La operación de convolución entre dos señales unidimensionales discretas es\n",
    "\n",
    "$$\n",
    "(f * g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[n-m]\n",
    "$$\n",
    "\n",
    "y la operación de correlación cruzada es\n",
    "\n",
    "$$\n",
    "(f \\star g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[m+n]\n",
    "$$\n",
    "\n",
    "> Para ambas operaciones el resultado es una nueva señal que también depende de  $n$\n",
    "\n",
    "\n",
    "\n",
    "Por ejemplo el elemento $0$ de $f\\star g$ se calcula como\n",
    "\n",
    "    f[0] g[0] + f[1] g[1] + f[2] g[2] + ...\n",
    "\n",
    "Luego el elemento $1$ sería\n",
    "\n",
    "    f[0] g[1] + f[1] g[2] + f[2] g[3] + ...\n",
    "    \n",
    "¿Cómo se ve esta operación graficamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "plt.close('all'); fig, ax = plt.subplots(2, figsize=(7, 4))\n",
    "ax2 = ax[0].twinx()\n",
    "data = subimg[0, :]\n",
    "\n",
    "def filt(k):\n",
    "    kernel = np.zeros(shape=(len(data),))\n",
    "    kernel[k:k+5] = 1\n",
    "    #kernel[k] = 1.; kernel[k+1] = -1.\n",
    "    return kernel\n",
    "\n",
    "true_filt = filt(0)[np.absolute(filt(0)) >0]\n",
    "display(true_filt)\n",
    "conv_s = scipy.signal.correlate(data, true_filt, mode='valid')\n",
    "\n",
    "\n",
    "def update(k): \n",
    "    ax[0].cla(); ax[1].cla(); ax2.cla();\n",
    "    ax[0].plot(data)\n",
    "    ax2.plot(filt(k), c='r')\n",
    "    ax[1].plot(conv_s); \n",
    "    ax[1].scatter(k, conv_s[k], s=100, c='k')\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=len(conv_s), interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de imágenes con convoluciones\n",
    "\n",
    "Se puede extender el concepto de convolución a dos dimensiones\n",
    "$$\n",
    "(I_1 * I_2) [n_1, n_2] = \\sum_{m_1=-\\infty}^\\infty \\sum_{m_2=-\\infty}^\\infty I_1[m_1, m_2] I_2[n_1-m_2, n_2 - m_2]\n",
    "$$\n",
    "\n",
    "donde $n_1$ es el índice de las filas y $n_2$ es el índice de las columnas\n",
    "\n",
    "#### La convolución entre dos imágenes es una nueva imagen\n",
    "\n",
    "La imagen $I_1$ es la entrada\n",
    "\n",
    "La imagen $I_2$ se denomina filtro o kernel de la convolución\n",
    "\n",
    "La imagen resultante es la imagen filtrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-bajo\n",
    "\n",
    "Suaviza, elimina los detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3\n",
    "filt = np.zeros(shape=(D, D))\n",
    "filt[1:-1, 1:-1] = 1\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt/np.sum(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-alto\n",
    "\n",
    "Resalta los cambios bruscos, elimina las partes \"planas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.array([[1., -1.]]*2)\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt, mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector de patillas\n",
    "\n",
    "Detecta patillas de fotografos mirando al horizonte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.ones(shape=(11, 11))\n",
    "filt[:9, 2:9] = 0\n",
    "display(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_res = scipy.signal.correlate2d(subimg, filt-np.mean(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "maxloc = np.unravel_index(np.argmax(img_res), shape=img_res.shape)\n",
    "ax[0].scatter(maxloc[1]+filt.shape[0]//2, maxloc[0]+filt.shape[1]//2, c='r', s=20)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Podríamos aprender filtros para detectar objetos específicos\n",
    "\n",
    "> Necesitamos aprender los valores de los \"píxeles\" del kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visión computacional\n",
    "\n",
    "La [visión computacional](http://szeliski.org/Book/) es un campo de investigación que busca que los computadores sean capaces de \"comprender\" el contenido presente en imágenes digitales y video\n",
    "\n",
    "#### Objetivo: \n",
    "\n",
    "> Automatizar tareas realizas por el sistema visual humano\n",
    "\n",
    "- Clasificación y Reconocimiento: ¿A qué categoría corresponde el patrón en la imagen?\n",
    "- Detección, Localización y Segmentación: ¿Dónde está el patrón en la imagen? \n",
    "- [Estimación de pose](https://modelzoo.co/blog/deep-learning-models-and-code-for-pose-estimation)\n",
    "- [Reconstrucción](https://www.youtube.com/watch?v=gg0F5JjKmhA), [super-resolución](https://www.extremetech.com/extreme/132950-csi-style-super-resolution-image-enlargment-yeeaaaah) y [síntesis](https://tcwang0509.github.io/pix2pixHD/)\n",
    "- ...\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/detection-and-segmentation-through-convnets-47aa42de27ea\"><img src=\"https://miro.medium.com/max/800/1*SNvD04dEFIDwNAqSXLQC_g.jpeg\" width=\"600\"></a>\n",
    "\n",
    "\n",
    "#### Aplicaciones\n",
    "\n",
    "- [Medicina](https://www.rsipvision.com/medical-segmentation/)\n",
    "- [Navegación autónoma](https://www.youtube.com/watch?v=H7Ym3DMSGms)\n",
    "- [Sistemas de control de tráfico](https://www.youtube.com/watch?v=jxhAWuImxS8)\n",
    "- [Realidad aumentada](https://www.youtube.com/watch?v=r9hVypi_6TQ)\n",
    "- [Agricultura y forestal](https://medium.com/@awangenh/mapping-weeds-and-crops-in-precision-agriculture-with-convolutional-neural-networks-138dab87ba00)\n",
    "- [...](https://www.cs.ubc.ca/~lowe/vision.html)\n",
    "\n",
    "#### Herramientas\n",
    "\n",
    "- Procesamiento digital de imágenes\n",
    "- Optimización, Estadística \n",
    "- Machine learning y en particular  **Redes Neuronales Convolucionales** \n",
    "\n",
    "\n",
    "#### Desafios\n",
    "\n",
    "- Algoritmos invariantes a los cambios de Iluminación\n",
    "- Algoritmos invariantes a los cambios de escala y perspectiva (deformación)\n",
    "- Algoritmos robustos contra la oclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales\n",
    "\n",
    "[Slides 52-92](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit#slide=id.g3a1a71fe7e_8_192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional en PyTorch\n",
    "\n",
    "Las redes neuronales convolucionales utilizan principalmente tres tipos de capas\n",
    "\n",
    "#### [Capas convolucionales](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "\n",
    "- Las neuronas de estas capas se organizan en filtros \n",
    "- Se realiza la correlación cruzada entre la imagen de entrada y los filtros\n",
    "- Existen capas convolucionales 1D, 2D y 3D\n",
    "- [Visualización de convoluciones con distintos tamaños, strides, padding, dilation](https://github.com/vdumoulin/conv_arithmetic)\n",
    "    \n",
    "    \n",
    "        torch.nn.Conv2d(in_channels, **Cantidad de canales de la imagen de entrada**\n",
    "                        out_channels, **Cantidad de bancos de filtro**\n",
    "                        kernel_size, **Tamaño de los filtros (entero o tupla)**\n",
    "                        stride=1, **Paso de los filtros**\n",
    "                        padding=0, **Cantidad de filas y columnas para agregar a los filtros **\n",
    "                        dilation=1, **Espacio entre los pixeles de los filtros**\n",
    "                        groups=1, **Configuración cruzada entre filtros de entrada y salida**\n",
    "                        bias=True,  **Utilizar sesgo (b)**\n",
    "                        padding_mode='zeros' **Especifica como agregar nuevas filas/columnas (ver padding)**\n",
    "                        )\n",
    "                      \n",
    "      \n",
    "#### [Capas de pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "\n",
    "- Capa para reducir la dimensión (tamaño) de la capa anterior\n",
    "- Esto reduce la complejidad del modelo y ayuda\n",
    "- Realiza una operación no entrenable: \n",
    "    - Máximo de los pixeles en una región (kernel_size=2, stride=2)\n",
    "    \n",
    "        \n",
    "        1 2 1 0\n",
    "        2 3 1 2      3 2\n",
    "        0 1 0 1      2 1\n",
    "        2 0 0 0\n",
    "        \n",
    "    - Promedio de los píxeles en una región (kernel_size=2, stride=2)\n",
    "    \n",
    "    \n",
    "        1 2 1 0\n",
    "        2 3 1 2      2.00 1.00\n",
    "        0 1 0 1      0.75 0.25\n",
    "        2 0 0 0\n",
    "\n",
    "    torch.nn.MaxPool2d(kernel_size, **Mismo significado que en Conv2d**\n",
    "                       stride=None, **Mismo significado que en Conv2d** \n",
    "                       padding=0, **Mismo significado que en Conv2d**\n",
    "                       dilation=1, **Mismo significado que en Conv2d**\n",
    "                       return_indices=False, **Solo necesario para hacer unpooling**\n",
    "                       ceil_mode=False **Usar ceil en lugar de floor para calcular el tamaño de la salida**\n",
    "                       )\n",
    "        \n",
    "#### [Capas completamente conectadas](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
    "\n",
    "- Idénticas a las usadas en redes tipo MLP\n",
    "- Realizan la operación: $Z = WX + b$\n",
    "\n",
    "        torch.nn.Linear(in_features, **Neuronas en la entrada\n",
    "                        out_features,  **Neuronas en la salida**\n",
    "                        bias=True  **Utilizar sesgo (b)**\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi primera red convolucional para clasificar en pytorch\n",
    "\n",
    "Clasificaremos la base de datos MNIST (10 clases)\n",
    "\n",
    "La arquitectura inicial considera\n",
    "- Una capa convolucional con 8 bancos de filtros\n",
    "- La capa convolucional espera un minibatch de imágenes de 1 canal (blanco y negro)\n",
    "- La capa convolucional usa filtros de 3x3 píxeles\n",
    "- Función de activación [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/nn.html#relu)\n",
    "- Una capa de max-pooling de tamaño 2x2 y stride 2\n",
    "- **Importante** La función `reshape` convierte el tensor de 4 dimensiones a uno de 2 dimensiones. La capa comple\n",
    "- Una capa completamente conectada con 10 neuronas de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "class mi_red_convolucional(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=20, n_filters=8):\n",
    "        super(mi_red_convolucional, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n_filters, out_channels=8, kernel_size=3)\n",
    "        self.mpool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=8*5*5, out_features=n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(in_features=n_hidden, out_features=10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.mpool2x2(self.activation(self.conv1(x)))\n",
    "        z = self.mpool2x2(self.activation(self.conv2(z)))\n",
    "        # print(z.shape)\n",
    "        z = z.reshape(-1, 8*5*5)\n",
    "        z = self.activation(self.fc1(z))\n",
    "        return self.fc2(z)\n",
    "    \n",
    "model = mi_red_convolucional()\n",
    "# model.forward(mnist_train_data[0][0].unsqueeze(0))\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación multiclase en PyTorch\n",
    "\n",
    "Para hacer clasificación con **más de dos categorías** usamos la [entropía cruzada](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "    torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Notemos que esto es distinto a la [entropía cruzada binaria](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss) que usamos para clasificar dos clases\n",
    "\n",
    "Si el problema de clasificación es de $M$ categorías la última capa de la red debe tener $M$ neuronas\n",
    "\n",
    "Adicionalmente no se debe usar función de activación ya que `CrossEntropyLoss` la aplica de forma interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de activación que se aplica de forma interna es la [SoftMax](https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax)\n",
    "\n",
    "SoftMax recibe $M$ valores y las aplica la transformación\n",
    "$$\n",
    "y_i = \\text{SoftMax}_i(x) = \\frac{e^{x_i}}{\\sum_{i=1}^M e^{x_i}}\n",
    "$$\n",
    "\n",
    "El resultado es que los $y_i \\in [0, 1]$ y además $\\sum_{i=1}^M y_i = 1$, es decir que lo podemos interpretar como una probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de activación en PyTorch\n",
    "\n",
    "En la red neuronal que acabamos de implementar abandonamos la función de activación sigmoide por la [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/nn.html#relu)\n",
    "\n",
    "Problema de las funciones de activación clásicas (sigmoide y tangente hiperbólica):\n",
    "> Extensas zonas saturadas (planas): Cero gradiente\n",
    "\n",
    "Con esto en mente se diseñó la función \n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Que solo satura en los números negativos\n",
    "\n",
    "> En la práctica usar ReLU acelera considerablemente el entrenamiento: más fácil de calcular y menos \"muertes\" de gradiente\n",
    "\n",
    "**Atención**\n",
    "\n",
    "Esta es una área de investigación activa\n",
    "\n",
    "Existen muchas funciones de activación que se han propuesto posterior a ReLU\n",
    "\n",
    "Recomendación: Partir con ReLU y luego probar [otras](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) (LeakyReLU, ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 4), tight_layout=True)\n",
    "\n",
    "activation = torch.nn.Sigmoid()\n",
    "y = activation(x)\n",
    "ax[0, 0].plot(x.numpy(), y.numpy());\n",
    "ax[0, 0].set_title('Sigmoide')\n",
    "\n",
    "activation = torch.nn.Tanh()\n",
    "y = activation(x)\n",
    "ax[1, 0].plot(x.numpy(), y.numpy());\n",
    "ax[1, 0].set_title('Tangente Hiperbólica')\n",
    "\n",
    "activation = torch.nn.ReLU()\n",
    "y = activation(x)\n",
    "ax[0, 1].plot(x.numpy(), y.numpy());\n",
    "ax[0, 1].set_title('ReLU')\n",
    "\n",
    "activation = torch.nn.ELU()\n",
    "y = activation(x)\n",
    "ax[1, 1].plot(x.numpy(), y.numpy());\n",
    "ax[1, 1].set_title('ELU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente con paso adaptivo\n",
    "\n",
    "Para acelerar el entrenamiento podemos usar un algoritmo de [gradiente descendente con paso adaptivo](https://arxiv.org/abs/1609.04747)\n",
    "\n",
    "Un ejemplo ampliamente usado es [Adam](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "- Se utiliza la historia de los gradientes\n",
    "- Se utiliza momentum (inercia)\n",
    "- Cada parámetro tiene un paso distinto\n",
    "\n",
    "    torch.optim.Adam(params,  **Parámetros de la red neuronal**\n",
    "                     lr=0.001,  **Tasa de aprendizaje inicial**\n",
    "                     betas=(0.9, 0.999),  **Factores de olvido de los gradientes históricos**\n",
    "                     eps=1e-08, **Término para evitar división por cero**\n",
    "                     weight_decay=0, **Regulariza los pesos de la red si es mayor que cero**\n",
    "                     amsgrad=False **Corrección para mejorar la convergencia de Adam en ciertos casos**\n",
    "                     )\n",
    "\n",
    "**Atención**\n",
    "\n",
    "Esta es un área de investigación activa\n",
    "\n",
    "Papers recientes indican que Adam llega a un óptimo más rápido que SGD, pero ese óptimo podría no ser mejor que el obtenido por SGD\n",
    "\n",
    "> Siempre prueba tus redes con distintos optimizadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html)\n",
    "\n",
    "Es una librería utilitaria de PyTorch que facilita considerablemente el trabajo con imágenes\n",
    "\n",
    "- Funcionalidad para descargar sets de benchmark: MNIST, CIFAR, IMAGENET, ...\n",
    "- Modelos clásicos pre-entrenados: Lenet5, AlexNet\n",
    "- Funciones para importar imágenes en distintos formatos\n",
    "- Funciones de transformación para hacer aumentación de datos en imágenes\n",
    "\n",
    "#### Instalación\n",
    "\n",
    "Usando conda o \n",
    "\n",
    "    pip3 install torchvision \n",
    "\n",
    "#### Ejemplo: Obtener base de datos de imágenes de dígitos manuscritos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                              train=True, download=True, \n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                             train=False, download=True, \n",
    "                                             transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imágenes de 28x28 píxeles en escala de grises\n",
    "- Diez categorías: Dígitos manuscritos del cero al nueve\n",
    "- 60.000 imágenes de entrenamiento, 10.000 imágenes de prueba\n",
    "- Por defecto las imágenes vienen en [formato PIL](https://pillow.readthedocs.io/en/stable/) (entero 8bit), usamos la transformación [`ToTensor()`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) para convertirla a tensor en float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_train_data[0]\n",
    "display(len(mnist_train_data), type(image), image.dtype, type(label))\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 1.5), tight_layout=True)\n",
    "idx = np.random.permutation(len(mnist_train_data))[:10]\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[idx[k]]\n",
    "    ax[k].imshow(image[0, :, :].numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders\n",
    "\n",
    "Creamos dataloaders de entrenamineto y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection\n",
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(mnist_train_data.data, mnist_train_data.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "train_dataset = Subset(mnist_train_data, train_idx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Data loader de validación\n",
    "valid_dataset = Subset(mnist_train_data, valid_idx)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red convolucional\n",
    "\n",
    "- Si tenemos acceso a una GPU podemos usar el atributo `.cuda()` o `.to()` para enviar el modelo y los datos a la GPU para acelerar los cálculos\n",
    "- Actualizamos los parámetros en el conjunto de entrenamiento\n",
    "- Medimos la convergencia en el conjunto de validación\n",
    "- Guardamos el modelo con mejor eror de validación\n",
    "- Usaremos [tqdm](https://tqdm.github.io/) para medir el tiempo por iteración dentro de jupyter. Instalar con [conda](https://anaconda.org/conda-forge/tqdm) o [pip](https://pypi.org/project/tqdm/)\n",
    "\n",
    "\n",
    "### Visualización de entrenamiento usando tensorboard\n",
    "\n",
    "Podemos usar la herramienta [tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) para visualizar el entrenamiento de la red en vivo y/o comparar distintos entrenamientos\n",
    "\n",
    "- Instalar tensorboard versión 1.15 o mayor con [conda](https://anaconda.org/conda-forge/tensorboard) o pip\n",
    "\n",
    "- Escribir en un terminal\n",
    "\n",
    "        tensorboard --logdir=/tmp/tensorboard/\n",
    "\n",
    "- Apuntar el navegador a \n",
    "\n",
    "        https://localhost:6006 \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "use_gpu = False\n",
    "nepochs = 100\n",
    "best_valid = np.inf\n",
    "writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\"+str(time.time_ns()),\n",
    "                       flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    nnet = nnet.cuda()\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)): \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer.step()\n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "    writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "    writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       '/home/phuijse/models/best_model.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    nnet = model.cpu()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluando la red en el conjunto de test\n",
    "\n",
    "Primero recuperamos la mejor red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mi_red_convolucional()\n",
    "\n",
    "model.load_state_dict(torch.load('/home/phuijse/models/best_model.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificar un ejemplo de test debemos pasar la salida por una activación SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_test_data[np.random.randint(1000)]\n",
    "# Usamos unsqueeze para convertirlo en un minibatch de 1 elemento:\n",
    "y = torch.nn.Softmax(dim=1)(model.forward(image.unsqueeze(0)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 3))\n",
    "ax[0].bar(range(10), height=y.detach().numpy()[0])\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[1].set_title(\"Etiqueta: %d\" %(label))\n",
    "ax[1].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r);\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar la red en el conjunto de prueba y construir una tabla de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(mnist_test_data, shuffle=False, batch_size=512)\n",
    "test_targets = mnist_test_data.targets.numpy()\n",
    "prediction_test = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits = model.forward(mbdata)\n",
    "    prediction_test.append(torch.argmax(logits, dim=1).detach().numpy())\n",
    "\n",
    "prediction_test = np.concatenate(prediction_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una tarea muy importante es analizar los errores de la red\n",
    "\n",
    "- Buscar ejemplos que estaban mal etiquetados\n",
    "- Proponer mejoras (nuevos atributos, capas, etc)\n",
    "\n",
    "Por ejemplo si visualizamos los que deberían ser $7$ pero son clasificados como otro dígito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((test_targets == 9) & ~(prediction_test == 9))[0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 2), tight_layout=True)\n",
    "for i in range(10):\n",
    "    ax[i].imshow(mnist_test_data[idx[i]][0].numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[i].set_title(prediction_test[idx[i]])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando los filtros aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(7, 2), tight_layout=True)\n",
    "w = model.conv1.weight.data.numpy()\n",
    "\n",
    "for i in range(8):    \n",
    "    ax[i].imshow(w[i, 0, :, :])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumentación de datos\n",
    "\n",
    "Si tenemos un dataset de imágenes muy pequeño y la red que estamos entrenando se está sobreajustando podemos intentar incrementarlo usando **transformaciones**\n",
    "\n",
    "Si rotamos, trasladamos o cambiamos el brillo de una imagen obtendremos una nueva imagen \"casi siempre\" de la misma clase\n",
    "\n",
    "*torchvision* tiene funciones implementadas para hacer transformaciones:\n",
    "\n",
    "- [Rotación aleatoria](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation)\n",
    "- [Espejamiento aleatorio](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip)\n",
    "- [Cropping aleatorio](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomCrop): Recortar la imagen\n",
    "- [Cambios aleatorios de brillo y contraste](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ColorJitter)\n",
    "- [Transformación afin aleatoria](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "- [entre otros](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "Cada transformación permite especificar límites, por ejemplo \"máximo ángulo de rotación\", \"máxima distorsión de brillo\", etc\n",
    "\n",
    "Las transformaciones también sirven para hacer que la red gane \"invarianzas\"\n",
    "\n",
    "**Ejemplo:** Si entrenamos con copias rotadas de nuestras imágenes, la red se volverá invariante a la rotación\n",
    "\n",
    "### Cuidado\n",
    "\n",
    "> Las transformaciones que apliquemos no deben cambiar la interpretación de clase\n",
    "\n",
    "- Si rotas un seis en 180 grados se convierte en un nueve\n",
    "- Si cambias demasiado el tono (hue) podrías obtener colores distintos a la realidad (perro verde?)\n",
    "\n",
    "### Transformaciones aleatorias con [torchvision](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "La mayoría de las transformaciones están diseñadas para aplicarse sobre imágenes en formato PIL\n",
    "\n",
    "Podemos componer varias transformaciones usando [`torchvision.transforms.Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://i.kym-cdn.com/photos/images/newsfeed/000/674/934/422.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"dog.jpg\")\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(200),\n",
    "                                            torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                            torchvision.transforms.RandomRotation(degrees=30),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                           ])\n",
    "\n",
    "display(torchvision.transforms.Resize(200)(img))\n",
    "display(transform(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando con datos aumentados\n",
    "\n",
    "Podemos componer una transformación y añadirla a un dataset\n",
    "\n",
    "Luego cuando usamos el dataloader se generaran imágenes con transformaciones aleatorias\n",
    "\n",
    "\n",
    "\n",
    "### IMPORTANTE: SOLO SE AUMENTA EL CONJUNTO DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.RandomAffine(degrees=30, translate=(0.2, 0.2), \n",
    "                                                                                scale=(0.5, 1.5), shear=None, \n",
    "                                                                                resample=False, fillcolor=0),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                            torchvision.transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                             train=True, download=True, \n",
    "                                             transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, shuffle=False, batch_size=32)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "\n",
    "fig, ax = plt.subplots(4, 8, figsize=(7, 4), tight_layout=True)\n",
    "for k in range(32):\n",
    "    i, j = np.unravel_index(k, (4, 8))\n",
    "    ax[i, j].axis('off')\n",
    "    ax[i, j].set_title(label[k].numpy())\n",
    "    ax[i, j].imshow(image[k].numpy()[0, :, :], cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando un modelo pre-entrenado\n",
    "\n",
    "[`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) ofrece una serie de modelos famosos de la literatura de *deep learning*\n",
    "\n",
    "Por defecto el modelo se carga con pesos aleatorios\n",
    "\n",
    "Se pueden escoger modelos para clasificar, localizar y segmentar\n",
    "\n",
    "Si indicamos `pretrained=True` se descarga un modelo entrenado\n",
    "\n",
    "Por ejemplo, podemos cargar [resnet18](https://arxiv.org/pdf/1512.03385.pdf) [pre-entrenado](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18) en [ImageNet](http://image-net.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_model = torchvision.models.resnet18(pretrained=True, progress=True)\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos pre-entrenados esperan imágenes con\n",
    "- tres canales (RGB)\n",
    "- al menos 224x224 píxeles\n",
    "- píxeles entre 0 y 1 (float)\n",
    "- normalizadas con \n",
    "\n",
    "        normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "img = Image.open(\"dog.jpg\")\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(256),\n",
    "                                            torchvision.transforms.CenterCrop(224),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "# La clase con probabilidad más alta\n",
    "resnet_model.forward(transform(img).unsqueeze(0)).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿A qué corresponde esta clase?\n",
    "\n",
    "Clases de ImageNet: https://gist.github.com/ageitgey/4e1342c10a71981d0b491e1b8227328b\n",
    "\n",
    "¿y la imagen que era?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "Aspectos a considerar durante el entrenamiento de redes neuronales\n",
    "- Arquitecturas (cantidad y organización de capas, funciones de activación)\n",
    "- Funciones de costo y Optimizadores (tasa de aprendizaje)\n",
    "- Verificar convergencia y sobreajuste:\n",
    "    - Siempre ir guardando el mejor modelo en validación (checkpoint)\n",
    "    - Detener el entrenamiento si el error de validación no disminuye \n",
    "- Inicialización de los parámetros: Probar varios entrenamientos desde inicios aleatorios distintos\n",
    "- Normalización de entrada y normalización capa a capa ([batch-normalization](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d))\n",
    "- Si el modelo se sobreajusta pronto\n",
    "    - Disminuir complejidad\n",
    "    - Regularizar (Aumentación de datos, penalización de los pesos, Dropout)\n",
    "- Si quiero aprovechar un modelo preentrenado\n",
    "    - Transferencia de aprendizaje\n",
    "    - [Zoológico de modelos](https://modelzoo.co/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Aprendizaje\n",
    "\n",
    "Ajuste fino de un modelo pre-entrenado\n",
    "\n",
    "FUTURO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localización y segmentación\n",
    "\n",
    "Encontrar y segmentar objetos en imágenes\n",
    "\n",
    "FUTURO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
